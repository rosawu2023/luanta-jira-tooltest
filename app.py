import streamlit as st
import pandas as pd
import plotly.express as px
from datetime import timezone

# =========================================================
# UI helpers (Bilingual + Empty state)
# =========================================================
def t(en: str, zh: str) -> str:
    return f"{en}ï¼ˆ{zh}ï¼‰"

def h1(en: str, zh: str):
    st.header(t(en, zh))

def h2(en: str, zh: str):
    st.subheader(t(en, zh))

def caption(en: str, zh: str):
    st.caption(f"{en} / {zh}")

def empty_state(
    zh: str,
    en: str = "",
    tips=None,
    level="info"
):
    msg = f"â„¹ï¸ {en}\n\n{zh}" if en else f"â„¹ï¸ {zh}"
    if level == "warning":
        st.warning(msg)
    else:
        st.info(msg)

    if tips:
        with st.expander(t("Possible reasons / Troubleshooting", "å¯èƒ½åŸå›  / æ’æŸ¥å»ºè­°"), expanded=False):
            for x in tips:
                st.write(f"- {x}")

def render_chart_or_empty(df_plot: pd.DataFrame, chart_fn, empty_zh: str, empty_en: str, tips=None):
    if df_plot is None or df_plot.empty:
        empty_state(empty_zh, empty_en, tips=tips, level="info")
        return
    fig = chart_fn(df_plot)
    st.plotly_chart(fig, use_container_width=True)

def safe_first_match(cols, keyword_list):
    """Return first column that contains any keyword (case-insensitive)."""
    lower_map = {c.lower(): c for c in cols}
    for kw in keyword_list:
        for c in cols:
            if kw.lower() in c.lower():
                return c
    return None

def to_datetime_safe(s):
    # Parse datetime robustly; return NaT for bad values
    return pd.to_datetime(s, errors="coerce", utc=True)

# =========================================================
# Page config
# =========================================================
st.set_page_config(page_title="Luanta Jira Analytics", layout="wide")
st.title("ğŸ“Š " + t("Luanta Service Performance Dashboard", "Luanta æœå‹™ç¸¾æ•ˆå„€è¡¨æ¿"))

# =========================================================
# Sidebar - Upload
# =========================================================
st.sidebar.header(t("Step 1: Upload Data", "æ­¥é©Ÿ 1ï¼šä¸Šå‚³è³‡æ–™"))
uploaded_file = st.sidebar.file_uploader("Upload Jira CSV / ä¸Šå‚³ Jira CSV", type="csv")

# Default data fallback: v1 first, then v0
df = None
default_loaded = None

if uploaded_file is None:
    # Try v1 then v0
    for fn in ["Luanta_Final_Demo_Data_v1.csv", "Luanta_Final_Demo_Data.csv"]:
        try:
            df = pd.read_csv(fn)
            default_loaded = fn
            break
        except Exception:
            pass

    if df is not None:
        st.success(f"âœ… {t('Loaded default sample data', 'å·²è¼‰å…¥é è¨­ç¯„ä¾‹è³‡æ–™')}ï¼š{default_loaded}")
        st.info("ğŸ’¡ " + t("You can upload a new CSV anytime from the sidebar.", "ä½ ä¹Ÿå¯ä»¥éš¨æ™‚åœ¨å·¦å´ä¸Šå‚³æ–°çš„ CSVã€‚"))
    else:
        st.warning("âš ï¸ " + t("Please upload a Jira CSV to start.", "è«‹å…ˆåœ¨å·¦å´ä¸Šå‚³ Jira CSV æ‰èƒ½é–‹å§‹åˆ†æã€‚"))
else:
    df = pd.read_csv(uploaded_file)
    st.success(f"âœ… {t('Successfully loaded uploaded file', 'å·²æˆåŠŸè®€å–ä¸Šå‚³æª”æ¡ˆ')}ï¼š{uploaded_file.name}")

if df is None:
    st.stop()

# =========================================================
# Debug / Preview
# =========================================================
with st.expander(t("Debug info", "é™¤éŒ¯è³‡è¨Š"), expanded=False):
    st.write(t("Detected columns", "åµæ¸¬åˆ°çš„æ¬„ä½") + ":", list(df.columns))
    st.write(t("Row count", "è³‡æ–™ç­†æ•¸") + ":", len(df))

with st.expander(t("Data preview (first 20 rows)", "è³‡æ–™é è¦½ï¼ˆå‰ 20 ç­†ï¼‰"), expanded=False):
    st.dataframe(df.head(20), use_container_width=True)

# =========================================================
# Column detection (robust)
# =========================================================
delay_col = safe_first_match(df.columns, ["delay_rate", "delay", "delay_rate_%"])
priority_col = "Priority" if "Priority" in df.columns else safe_first_match(df.columns, ["priority"])
role_col = "Role" if "Role" in df.columns else safe_first_match(df.columns, ["role", "team"])
status_col = "Status_Current" if "Status_Current" in df.columns else safe_first_match(df.columns, ["status_current", "status"])
last_updated_col = "Last_Updated_Date" if "Last_Updated_Date" in df.columns else safe_first_match(df.columns, ["last_updated", "updated"])
status_entered_col = "Status_Entered_Date" if "Status_Entered_Date" in df.columns else safe_first_match(df.columns, ["status_entered", "entered_date"])
issue_key_col = "Issue_Key" if "Issue_Key" in df.columns else safe_first_match(df.columns, ["issue_key", "key"])
summary_col = "Summary" if "Summary" in df.columns else safe_first_match(df.columns, ["summary", "title"])
assignee_col = "Assignee" if "Assignee" in df.columns else safe_first_match(df.columns, ["assignee", "owner"])
blocked_reason_col = "Blocked_Reason" if "Blocked_Reason" in df.columns else safe_first_match(df.columns, ["blocked_reason", "block_reason", "blocked"])
root_cause_col = "Root_Cause_Category" if "Root_Cause_Category" in df.columns else safe_first_match(df.columns, ["root_cause", "cause"])

# =========================================================
# Key Metrics
# =========================================================
h1("Key Metrics", "é—œéµæŒ‡æ¨™")
caption("High-level KPIs for quick health check.", "ç”¨ 3 å€‹æ•¸å­—å¿«é€Ÿåˆ¤æ–·æ•´é«”å¥åº·åº¦ã€‚")

m1, m2, m3 = st.columns(3)

total_tickets = len(df)
m1.metric(t("Total Tickets", "ç¸½å·¥å–®æ•¸"), total_tickets)

# P0 count
p0_count = "N/A"
if priority_col and priority_col in df.columns:
    # Common P0 labels: "P0-Critical", "P0", "Critical"
    p0_count = int(df[priority_col].astype(str).str.contains("P0", case=False, na=False).sum())
m2.metric(t("P0 Critical Issues", "P0 é‡å¤§å·¥å–®æ•¸"), p0_count)

avg_delay = "N/A"
if delay_col and delay_col in df.columns:
    avg_delay_val = pd.to_numeric(df[delay_col], errors="coerce").mean()
    if pd.notna(avg_delay_val):
        avg_delay = f"{avg_delay_val:.1f}%"
m3.metric(t("Avg. Delay Rate", "å¹³å‡å»¶é²ç‡"), avg_delay)

# =========================================================
# Performance Breakdown
# =========================================================
h1("Performance Breakdown", "ç¸¾æ•ˆæ‹†è§£")
caption("Break down delay by role/team to locate bottlenecks.", "ä¾è§’è‰²/åœ˜éšŠæ‹†è§£å»¶é²ï¼Œå®šä½ç“¶é ¸ã€‚")

if not role_col or not delay_col or role_col not in df.columns or delay_col not in df.columns:
    empty_state(
        zh="ç›®å‰è³‡æ–™ä¸è¶³ä»¥é¡¯ç¤ºã€Œè§’è‰²å»¶é²æ‹†è§£ã€ã€‚éœ€è¦è‡³å°‘åŒ…å« Role èˆ‡ Delay Rate æ¬„ä½ã€‚",
        en="Not enough data to show delay breakdown by role. Need Role and Delay Rate columns.",
        tips=[
            "ç¢ºèª CSV æ˜¯å¦åŒ…å« Roleï¼ˆè§’è‰²/åœ˜éšŠï¼‰",
            f"ç¢ºèªæ˜¯å¦åŒ…å«å»¶é²æ¬„ä½ï¼ˆä¾‹å¦‚ {delay_col or 'Delay_Rate_%'}ï¼‰",
        ],
    )
else:
    perf_df = (
        df[[role_col, delay_col]]
        .copy()
    )
    perf_df[delay_col] = pd.to_numeric(perf_df[delay_col], errors="coerce")
    perf_df = perf_df.dropna(subset=[role_col, delay_col])
    perf_df = perf_df.groupby(role_col, as_index=False)[delay_col].mean().sort_values(delay_col, ascending=False)

    render_chart_or_empty(
        perf_df,
        chart_fn=lambda d: px.bar(
            d, x=role_col, y=delay_col, color=role_col,
            labels={delay_col: "Delay Rate (%)"},
            title="Average Delay by Team Role"
        ),
        empty_zh="ç›®å‰è³‡æ–™ä¸è¶³ä»¥è¨ˆç®—ã€Œè§’è‰²å¹³å‡å»¶é²ã€ã€‚å¸¸è¦‹åŸå› ï¼šå»¶é²æ¬„ä½ç‚ºç©ºæˆ–ç„¡æ³•è½‰æˆæ•¸å­—ã€‚",
        empty_en="Not enough numeric values to compute average delay by role.",
        tips=[
            "ç¢ºèª Delay æ¬„ä½æ˜¯å¦ç‚ºæ•¸å­—ï¼ˆä¾‹å¦‚ 12.3ï¼‰",
            "é¿å…æ··å…¥ % ç¬¦è™Ÿæˆ–æ–‡å­—ï¼ˆå¦‚ '12.3%'ï¼‰",
        ],
    )

# =========================================================
# Workflow Finder
# =========================================================
h1("Workflow Finder", "å·¥ä½œæµç¨‹åˆ†æ")
caption("WIP and queue time to identify where the workflow is stuck.", "ç”¨åœ¨æ‰‹é‡èˆ‡ç­‰å¾…æ™‚é–“æ‰¾å‡ºæµç¨‹å¡é»ã€‚")

c1, c2 = st.columns(2)

# ---------- WIP by Status ----------
with c1:
    h2("WIP by Status", "å„ç‹€æ…‹åœ¨æ‰‹é‡")
    caption("Shows current ticket distribution across statuses.", "é¡¯ç¤ºå·¥å–®ç›®å‰åˆ†ä½ˆåœ¨å“ªäº›ç‹€æ…‹ã€‚")

    if not status_col or status_col not in df.columns:
        empty_state(
            zh="ç›®å‰è³‡æ–™ä¸è¶³ä»¥è¨ˆç®—ã€Œå„ç‹€æ…‹åœ¨æ‰‹é‡ã€ã€‚å¸¸è¦‹åŸå› ï¼šç¼ºå°‘ Status_Current æ¬„ä½ï¼Œæˆ–æ¬„ä½å…¨ç‚ºç©ºã€‚",
            en="Not enough data to compute WIP by status.",
            tips=[
                "ç¢ºèª CSV æ˜¯å¦åŒ…å«æ¬„ä½ï¼šStatus_Current",
                "ç¢ºèª Status_Current æ˜¯å¦æœ‰å€¼ï¼ˆä¸æ˜¯å…¨éƒ¨ç©ºç™½ï¼‰",
            ],
        )
    else:
        wip_df = (
            df[status_col]
            .dropna()
            .value_counts()
            .rename_axis("Status")
            .reset_index(name="Count")
        )
        render_chart_or_empty(
            wip_df,
            chart_fn=lambda d: px.bar(d, x="Status", y="Count", title="Current Work In Progress by Status"),
            empty_zh="ç›®å‰æ²’æœ‰å¯ç”¨çš„ç‹€æ…‹è³‡æ–™å¯é¡¯ç¤ºï¼ˆç‹€æ…‹å¯èƒ½å…¨ç‚ºç©ºï¼‰ã€‚",
            empty_en="No usable status values found.",
            tips=["ç¢ºèª Status_Current æ¬„ä½æ˜¯å¦æœ‰å¡«å…¥ç‹€æ…‹å€¼ï¼ˆTo Do / In Progress / Review / Done ç­‰ï¼‰"],
        )

# ---------- Queue Time by Status ----------
with c2:
    h2("Queue Time (days) by Status", "å„ç‹€æ…‹ç­‰å¾…æ™‚é–“ï¼å¤©")
    caption("Average time tickets stay in each status (queue time).", "å„ç‹€æ…‹å¹³å‡åœç•™æ™‚é–“ï¼Œç”¨ä¾†æ‰¾æœ€æ…¢ç¯€é»ã€‚")

    # Compute queue days if possible
    q_df = pd.DataFrame()

    if status_col and status_entered_col and status_col in df.columns and status_entered_col in df.columns:
        tmp = df[[status_col, status_entered_col]].copy()
        tmp[status_entered_col] = to_datetime_safe(tmp[status_entered_col])

        # Use "now" in UTC for consistent calc (demo data might be old; still OK for illustration)
        now_utc = pd.Timestamp.now(tz=timezone.utc)
        tmp["_queue_days"] = (now_utc - tmp[status_entered_col]).dt.total_seconds() / 86400.0
        tmp = tmp.dropna(subset=[status_col, "_queue_days"])

        q_df = tmp.groupby(status_col, as_index=False)["_queue_days"].mean().sort_values("_queue_days", ascending=False)

    render_chart_or_empty(
        q_df,
        chart_fn=lambda d: px.bar(d, x=status_col, y="_queue_days", title="Average Queue Time by Status (days)"),
        empty_zh="ç›®å‰è³‡æ–™ä¸è¶³ä»¥è¨ˆç®—ã€Œç­‰å¾…æ™‚é–“ï¼ˆQueue Timeï¼‰ã€ã€‚å¸¸è¦‹åŸå› ï¼šç¼ºå°‘ç‹€æ…‹é€²å…¥æ™‚é–“ï¼ˆä¾‹å¦‚ Status_Entered_Dateï¼‰ï¼Œæˆ–æ—¥æœŸæ ¼å¼ç„¡æ³•è§£æã€‚",
        empty_en="Not enough data to compute queue time by status.",
        tips=[
            "ç¢ºèª CSV æ˜¯å¦åŒ…å«ï¼šStatus_Entered_Dateï¼ˆé€²å…¥ç›®å‰ç‹€æ…‹çš„æ™‚é–“ï¼‰",
            "æ—¥æœŸæ ¼å¼å»ºè­°ç”¨ ISOï¼ˆä¾‹å¦‚ 2025-01-01 18:34:25+00:00ï¼‰",
        ],
    )

# =========================================================
# Stale Tickets
# =========================================================
h1("Stale Tickets (no update)", "ä¹…æœªæ›´æ–°å·¥å–®")
caption("Tickets that haven't been updated for a long time; often indicates workflow stuck.", "é•·æ™‚é–“æœªæ›´æ–°ï¼Œå¸¸ä»£è¡¨æµç¨‹å¡ä½æˆ–ç¼ºä¹æ¨é€²ã€‚")

stale_df = pd.DataFrame()

if last_updated_col and last_updated_col in df.columns:
    tmp = df.copy()
    tmp[last_updated_col] = to_datetime_safe(tmp[last_updated_col])
    now_utc = pd.Timestamp.now(tz=timezone.utc)
    tmp["_stale_days"] = (now_utc - tmp[last_updated_col]).dt.total_seconds() / 86400.0

    # Default threshold: 14 days
    threshold = st.slider(t("Stale threshold (days)", "ä¹…æœªæ›´æ–°é–€æª»ï¼ˆå¤©ï¼‰"), 1, 180, 30)

    stale_df = tmp.dropna(subset=["_stale_days"])
    stale_df = stale_df[stale_df["_stale_days"] >= threshold].sort_values("_stale_days", ascending=False)

    show_cols = []
    for c in [issue_key_col, summary_col, priority_col, role_col, status_col, last_updated_col, "_stale_days"]:
        if c and c in stale_df.columns:
            show_cols.append(c)

    if stale_df.empty:
        empty_state(
            zh="ç›®å‰æ²’æœ‰ç¬¦åˆã€Œä¹…æœªæ›´æ–°ã€é–€æª»çš„å·¥å–®ï¼Œæˆ–è³‡æ–™ä¸­çš„æ›´æ–°æ™‚é–“ä¸è¶³ä»¥è¨ˆç®—ã€‚",
            en="No tickets exceed the stale threshold, or timestamps are missing.",
            tips=["å¦‚æœä½ æœŸå¾…çœ‹åˆ°è³‡æ–™ï¼Œè«‹ç¢ºèª Last_Updated_Date æ˜¯å¦å­˜åœ¨ä¸”å¯è§£æç‚ºæ—¥æœŸã€‚"],
        )
    else:
        st.dataframe(stale_df[show_cols], use_container_width=True)

else:
    empty_state(
        zh="ç›®å‰è³‡æ–™ä¸è¶³ä»¥è¨ˆç®—ã€Œä¹…æœªæ›´æ–°å·¥å–®ã€ã€‚å¸¸è¦‹åŸå› ï¼šç¼ºå°‘ Last_Updated_Date æ¬„ä½ï¼Œæˆ–æ—¥æœŸæ ¼å¼ç„¡æ³•è§£æã€‚",
        en="Not enough data to compute stale tickets. Missing Last_Updated_Date or invalid datetime format.",
        tips=[
            "ç¢ºèª CSV æ˜¯å¦åŒ…å«ï¼šLast_Updated_Date",
            "æ—¥æœŸæ ¼å¼å»ºè­°ç”¨ ISOï¼ˆä¾‹å¦‚ 2025-01-01 18:34:25+00:00ï¼‰",
        ],
    )

# =========================================================
# SLA / Risk
# =========================================================
h1("SLA / Risk", "SLA èˆ‡é¢¨éšª")
caption("Estimate SLA breach risk; depends on whether SLA columns exist.", "ä¼°ç®— SLA é•ç´„é¢¨éšªï¼›éœ€è¦æœ‰ SLA ç›¸é—œæ¬„ä½ã€‚")

sla_df = pd.DataFrame()
sla_breach_rate = None

# Option A: SLA_Breached exists
sla_breached_col = "SLA_Breached" if "SLA_Breached" in df.columns else safe_first_match(df.columns, ["sla_breached", "breach"])

# Option B: Resolution_Days + SLA_Target_Days exist
resolution_days_col = "Resolution_Days" if "Resolution_Days" in df.columns else safe_first_match(df.columns, ["resolution_days", "resolution_time_days", "resolution"])
sla_target_col = "SLA_Target_Days" if "SLA_Target_Days" in df.columns else safe_first_match(df.columns, ["sla_target_days", "sla_days", "sla_target"])

tmp = df.copy()

if sla_breached_col and sla_breached_col in tmp.columns:
    # Normalize boolean-ish values
    tmp[sla_breached_col] = tmp[sla_breached_col].astype(str).str.lower().isin(["true", "1", "yes", "y"])
    if priority_col and priority_col in tmp.columns:
        sla_df = tmp.groupby(priority_col, as_index=False)[sla_breached_col].mean()
        sla_df["Breach Rate (%)"] = (sla_df[sla_breached_col] * 100).round(2)
        sla_breach_rate = float(tmp[sla_breached_col].mean() * 100)

elif resolution_days_col and sla_target_col and resolution_days_col in tmp.columns and sla_target_col in tmp.columns:
    tmp[resolution_days_col] = pd.to_numeric(tmp[resolution_days_col], errors="coerce")
    tmp[sla_target_col] = pd.to_numeric(tmp[sla_target_col], errors="coerce")
    tmp["_sla_breached"] = (tmp[resolution_days_col] > tmp[sla_target_col])

    if priority_col and priority_col in tmp.columns:
        sla_df = tmp.groupby(priority_col, as_index=False)["_sla_breached"].mean()
        sla_df["Breach Rate (%)"] = (sla_df["_sla_breached"] * 100).round(2)
        sla_breach_rate = float(tmp["_sla_breached"].mean() * 100)

if sla_breach_rate is None:
    empty_state(
        zh="ç›®å‰è³‡æ–™ä¸è¶³ä»¥è¨ˆç®— SLA é•ç´„ç‡ã€‚å¸¸è¦‹åŸå› ï¼šç¼ºå°‘ SLA_Breachedï¼Œæˆ–ç¼ºå°‘ Resolution_Days + SLA_Target_Daysã€‚",
        en="Not enough data to compute SLA breach rate. Missing SLA_Breached or (Resolution_Days + SLA_Target_Days).",
        tips=[
            "å¦‚æœä½ å¸Œæœ›è¨ˆç®— SLAï¼Œå»ºè­°åœ¨ CSV åŠ å…¥ SLA_Target_Days èˆ‡ Resolution_Daysï¼ˆæˆ–ç›´æ¥æä¾› SLA_Breachedï¼‰ã€‚",
        ],
    )
else:
    st.write(f"**{t('SLA Breach Rate', 'SLA é•ç´„ç‡')}**: {sla_breach_rate:.1f}%")
    if not sla_df.empty:
        show_cols = [priority_col, "Breach Rate (%)"] if (priority_col and priority_col in sla_df.columns) else ["Breach Rate (%)"]
        st.dataframe(sla_df[show_cols], use_container_width=True)

# =========================================================
# Root Cause Breakdown
# =========================================================
h1("Root Cause Breakdown", "æ ¹å› åˆ†ä½ˆ")
caption("Aggregate probable root causes to inform process improvements.", "å½™æ•´æ ¹å› ï¼ŒæŒ‡å‘æµç¨‹æ”¹å–„èˆ‡åˆ¶åº¦åŒ–ã€‚")

if not root_cause_col or root_cause_col not in df.columns:
    empty_state(
        zh="ç›®å‰è³‡æ–™ä¸è¶³ä»¥é¡¯ç¤ºæ ¹å› åˆ†ä½ˆã€‚å¸¸è¦‹åŸå› ï¼šç¼ºå°‘ Root_Cause_Category æ¬„ä½ã€‚",
        en="Not enough data to show root cause distribution. Missing Root_Cause_Category.",
        tips=[
            "å»ºè­°åœ¨ CSV åŠ å…¥ Root_Cause_Categoryï¼ˆä¾‹å¦‚ Spec/Requirementã€API Dependencyã€Data/DB ç­‰ï¼‰",
        ],
    )
else:
    rc = df[root_cause_col].dropna()
    if rc.empty:
        empty_state(
            zh="æ ¹å› æ¬„ä½å­˜åœ¨ï¼Œä½†ç›®å‰æ²’æœ‰å¯ç”¨å€¼ï¼ˆå¯èƒ½å…¨ç‚ºç©ºï¼‰ã€‚",
            en="Root cause column exists but contains no usable values.",
            tips=["ç¢ºèª Root_Cause_Category æ˜¯å¦æœ‰å¡«å€¼ã€‚"],
        )
    else:
        rc_df = rc.value_counts().reset_index()
        rc_df.columns = ["Root Cause", "Count"]
        rc_df["Share (%)"] = (rc_df["Count"] / rc_df["Count"].sum() * 100).round(1)

        render_chart_or_empty(
            rc_df,
            chart_fn=lambda d: px.pie(d, names="Root Cause", values="Count", title="Root Cause Distribution"),
            empty_zh="ç›®å‰ç„¡æ³•é¡¯ç¤ºæ ¹å› åˆ†ä½ˆï¼ˆè³‡æ–™å¯èƒ½ä¸è¶³ï¼‰ã€‚",
            empty_en="Unable to render root cause distribution.",
        )

# =========================================================
# Blocked Details
# =========================================================
h1("Blocked Details", "å¡é—œæ˜ç´°")
caption("Tickets currently blocked and the reasons; good for action items.", "åˆ—å‡ºç›®å‰å¡é—œå·¥å–®èˆ‡åŸå› ï¼Œæ–¹ä¾¿é–‹æœƒæ¨é€²ã€‚")

blocked_df = pd.DataFrame()
if status_col and status_col in df.columns:
    blocked_df = df[df[status_col].astype(str).str.lower().eq("blocked")].copy()

# If no explicit status=Blocked, fallback: blocked reason exists
if blocked_df.empty and blocked_reason_col and blocked_reason_col in df.columns:
    blocked_df = df[df[blocked_reason_col].notna()].copy()

if blocked_df.empty:
    empty_state(
        zh="ç›®å‰æ²’æœ‰å¯è¾¨è­˜çš„å¡é—œå·¥å–®ï¼ˆBlockedï¼‰ã€‚è‹¥ä½ é æœŸæœ‰è³‡æ–™ï¼Œå¯èƒ½æ˜¯ç‹€æ…‹å‘½åä¸åŒæˆ–ç¼ºå°‘ Blocked_Reasonã€‚",
        en="No blocked tickets detected. Status naming may differ or Blocked_Reason is missing.",
        tips=[
            "ç¢ºèª Status_Current æ˜¯å¦åŒ…å« 'Blocked' ç‹€æ…‹",
            "æˆ–åœ¨ CSV æä¾› Blocked_Reason æ¬„ä½ä»¥åˆ©è¾¨è­˜",
        ],
    )
else:
    st.write(f"**{t('Blocked tickets', 'å¡é—œå·¥å–®æ•¸')}**: {len(blocked_df)}")

    cols = []
    for c in [issue_key_col, summary_col, priority_col, role_col, assignee_col, blocked_reason_col, status_col]:
        if c and c in blocked_df.columns:
            cols.append(c)

    st.dataframe(blocked_df[cols].head(50), use_container_width=True)

# =========================================================
# Executive Summary
# =========================================================
h1("Executive Summary", "ä¸»ç®¡æ‘˜è¦")
caption("Auto-generated talking points for managers; neutral and action-oriented.", "è‡ªå‹•ç”Ÿæˆä¸»ç®¡å¯ç”¨çš„é‡é»æ‘˜è¦ï¼šä¸­æ€§ã€å¯è¡Œå‹•ã€‚")

if st.button(t("Generate Executive Report", "ç”¢å‡ºä¸»ç®¡æ‘˜è¦")):
    bullets = []

    # Delay bottleneck
    if role_col and delay_col and role_col in df.columns and delay_col in df.columns:
        tmp = df[[role_col, delay_col]].copy()
        tmp[delay_col] = pd.to_numeric(tmp[delay_col], errors="coerce")
        tmp = tmp.dropna(subset=[role_col, delay_col])
        if not tmp.empty:
            r = tmp.groupby(role_col)[delay_col].mean().sort_values(ascending=False)
            top_role = r.index[0]
            bullets.append(f"- **{t('Delay Bottleneck', 'å»¶é²ç“¶é ¸')}**: {top_role} {t('has the highest average delay', 'å¹³å‡å»¶é²æœ€é«˜')} ({r.iloc[0]:.1f}%).")
        else:
            bullets.append(f"- **{t('Delay Bottleneck', 'å»¶é²ç“¶é ¸')}**: {t('Insufficient numeric delay data', 'å»¶é²æ¬„ä½ç„¡æœ‰æ•ˆæ•¸å­—è³‡æ–™')}ã€‚")
    else:
        bullets.append(f"- **{t('Delay Bottleneck', 'å»¶é²ç“¶é ¸')}**: {t('Missing Role/Delay columns', 'ç¼ºå°‘ Role æˆ– Delay æ¬„ä½')}ã€‚")

    # Queue bottleneck
    if status_col and status_entered_col and status_col in df.columns and status_entered_col in df.columns:
        tmp = df[[status_col, status_entered_col]].copy()
        tmp[status_entered_col] = to_datetime_safe(tmp[status_entered_col])
        now_utc = pd.Timestamp.now(tz=timezone.utc)
        tmp["_queue_days"] = (now_utc - tmp[status_entered_col]).dt.total_seconds() / 86400.0
        tmp = tmp.dropna(subset=[status_col, "_queue_days"])
        if not tmp.empty:
            q = tmp.groupby(status_col)["_queue_days"].mean().sort_values(ascending=False)
            top_status = q.index[0]
            bullets.append(f"- **{t('Queue Bottleneck', 'æµç¨‹ç­‰å¾…ç“¶é ¸')}**: {top_status} {t('has the longest average queue time', 'å¹³å‡ç­‰å¾…æ™‚é–“æœ€é•·')} ({q.iloc[0]:.1f} {t('days', 'å¤©')}).")
        else:
            bullets.append(f"- **{t('Queue Bottleneck', 'æµç¨‹ç­‰å¾…ç“¶é ¸')}**: {t('No usable queue-time values', 'ç„¡å¯ç”¨ç­‰å¾…æ™‚é–“æ•¸å€¼')}ã€‚")
    else:
        bullets.append(f"- **{t('Queue Bottleneck', 'æµç¨‹ç­‰å¾…ç“¶é ¸')}**: {t('Missing Status_Entered_Date (queue-time) field', 'ç¼ºå°‘ Status_Entered_Date ç„¡æ³•è¨ˆç®—ç­‰å¾…æ™‚é–“')}ã€‚")

    # SLA risk
    if sla_breach_rate is not None:
        bullets.append(f"- **{t('SLA Risk', 'SLA é¢¨éšª')}**: {t('Breach rate is', 'é•ç´„ç‡ç‚º')} {sla_breach_rate:.1f}%.")
    else:
        bullets.append(f"- **{t('SLA Risk', 'SLA é¢¨éšª')}**: {t('SLA fields not available; risk not computed', 'ç¼ºå°‘ SLA æ¬„ä½ï¼Œæœªè¨ˆç®—é¢¨éšª')}ã€‚")

    # Root cause
    if root_cause_col and root_cause_col in df.columns:
        rc = df[root_cause_col].dropna()
        if not rc.empty:
            top_cause = rc.value_counts().index[0]
            bullets.append(f"- **{t('Primary Root Cause', 'ä¸»è¦æ ¹å› ')}**: {top_cause} {t('is the most frequent category', 'ç‚ºæœ€å¸¸è¦‹åˆ†é¡')}ã€‚")
        else:
            bullets.append(f"- **{t('Primary Root Cause', 'ä¸»è¦æ ¹å› ')}**: {t('Root-cause values are empty', 'æ ¹å› æ¬„ä½ç›®å‰ç„¡æœ‰æ•ˆå€¼')}ã€‚")
    else:
        bullets.append(f"- **{t('Primary Root Cause', 'ä¸»è¦æ ¹å› ')}**: {t('Missing Root_Cause_Category', 'ç¼ºå°‘ Root_Cause_Category æ¬„ä½')}ã€‚")

    st.markdown("\n".join(bullets))
    st.caption(t("Tip: Use these bullets in a weekly review or exec update.", "æç¤ºï¼šå¯ç›´æ¥è²¼åˆ°é€±å ±/ä¸»ç®¡æ›´æ–°/è·¨éƒ¨é–€æœƒè­°ç´€éŒ„ã€‚"))
